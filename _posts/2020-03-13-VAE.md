<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# 变分自编码研究

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/generative.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图１：生成模型</div>
</center>

VAE是目前最为火热的非监督学习研究方向，这篇博文是本人对变分自编码的阅读和相关理解。变分自编码是一种生成模型。如图１所示，对于生成模型，我们的目标在于利用一些潜在的变量$z$去生成真实的数据$X$。比如说我们想要生成０到９的图像，这个潜在变量$z$可以是0到9的数字。这个生成的过程可以用一个映射$f(z；\theta)$来表示。$z$是潜在变量，而$\theta$是决定映射关系的参数。用概率来表示，我们有

\begin{equation}
P(X)=\int P(X | z;\theta)P(z)dz
\end{equation}

其中$P(X|z; \theta)$由$f(z；\theta)$决定。一个生成模型有两个问题，一是如何建模$z$，$z$应该是什么样的分布，另一个就是如何去确定$f(；\theta)$。确定$f(；\theta)$并不容易，因为牵涉到了对$z$的积分。在最基础的VAE之中，$z$被建模为标准分布$N(0,I)$。而$f(z；\theta)$被构建成了一个深度的神经网络，神经网络的多层表征可以将$z$映射到具有语义特性的表达上。如果$z \sim N(0,I)$，那么$ P(X | z;\theta) \sim N(X | f(z；\theta), \sigma^2 \ast I)$

To be 





