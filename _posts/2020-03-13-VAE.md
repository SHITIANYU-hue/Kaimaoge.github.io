<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# 变分自编码研究

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/generative.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图１：生成模型</div>
</center>

VAE是目前最为火热的非监督学习研究方向，这篇博文是本人对变分自编码的阅读和相关理解。变分自编码是一种生成模型。如图１所示，对于生成模型，我们的目标在于利用一些潜在的变量$z$去生成真实的数据$X$。比如说我们想要生成０到９的图像，这个潜在变量$z$可以是0到9的数字。这个生成的过程可以用一个映射$f(z；\theta)$来表示。$z$是潜在变量，而$\theta$是决定映射关系的参数。用概率来表示，我们有

\begin{equation}
P(X)=\int P(X | z;\theta)P(z)dz
\end{equation}

一个生成模型有两个问题需要解决，一是如何建模$z$，$z$应该是什么样的分布，另一个就是如何去确定$f(；\theta)$。确定$f(；\theta)$并不容易，因为牵涉到了对$z$的积分。在最基础的VAE之中，$z$被建模为标准分布$N(0,I)$。而$f(z；\theta)$被构建成了一个深度的神经网络，神经网络的多层表征可以将$z$映射到具有语义特性的表达上。如果$z \sim N(0,I)$，那么

\begin{equation}
P(X | z;\theta) \sim N(X | f(z;\theta), \sigma^2 \ast I)
\end{equation}

在深度学习研究中，如果我们可以找到$P(X)$的数学表达式，就可以利用梯度下降对其求解。一种方式是去采样$n$个$(z_i,z_2,\cdots,z_n)$，然后

\begin{equation}
P(X) = \frac{1}{n} P(X_i | z_i)
\end{equation}

这一方式对于复杂的数据集并不可行，我们可能需要采样大量的$z$才能对数据进行准确的估计。那么对于给定数据X，我们可以采样出最可能产生X的z吗？在VAE中，这个采样的概率被表达为Q(z)，而VAE的损失函数则可从Q(z)和$P(z\|X)$之间的KL散度推导得到。

\begin{equation}
KL(Q(z) || P(z|X))　= E_{z \sim Q}(logQ(z) - logP(z|X))
\end{equation}

\begin{equation}
= E_{z \sim Q}(logQ(z) - logP(X|z) - logP(z)) + P(x)
\end{equation}

\begin{equation}
logP(x) - KL(Q(z) || P(z|X)) = E_{z \sim Q}(logP(X|z)) - KL(Q(z) || P(z))
\end{equation}

我们可以以任意方式建模Q(z)，在这里，我们将Q(z)建模为由X决定的分布$Q(z\|X)$，那么等式变为以下公式

\begin{equation}
logP(x) - KL(Q(z|X) || P(z|X)) = E_{z \sim Q}(logP(X|z)) - KL(Q(z|x) || P(z))
\end{equation}

通过优化以上等式左侧的表达式，我们可以得到最大化logP(x)，即生成数据的可能性；另外最小化了$Q(z\|X)$和$P(z\|X)$的KL散度，即$Q(z\| X)$和$P(z\|X)$尽可能的接近了。而等式右侧的部分可以通过梯度下降求解。

To be continued





